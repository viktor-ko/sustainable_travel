{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a82fc256b829e8a",
   "metadata": {},
   "source": "## Get coordinates for selected cities"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "coordinates_file = 'data/coordinates.csv'\n",
    "\n",
    "def get_coordinates(city, country, username='v_kochk'):\n",
    "    url = f'http://api.geonames.org/searchJSON?q={city}&country={country}&maxRows=1&username={username}'\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data['geonames']:\n",
    "        city_data = data['geonames'][0]\n",
    "        return city_data['lat'], city_data['lng']\n",
    "    return None\n",
    "\n",
    "sel_cities = [\n",
    "    ('Vienna', 'AT'),\n",
    "    ('Brussels', 'BE'),\n",
    "    ('Sofia', 'BG'),\n",
    "    ('Zagreb', 'HR'),\n",
    "    ('Prague', 'CZ'),\n",
    "    ('Copenhagen', 'DK'),\n",
    "    ('Tallinn', 'EE'),\n",
    "    ('Paris', 'FR'),\n",
    "    ('Berlin', 'DE'),\n",
    "    ('Dresden', 'DE'),\n",
    "    ('Munich', 'DE'),\n",
    "    ('Budapest', 'HU'),\n",
    "    ('Rome', 'IT'),\n",
    "    ('Riga', 'LV'),\n",
    "    ('Vilnius', 'LT'),\n",
    "    ('Luxembourg', 'LU'),\n",
    "    ('Amsterdam', 'NL'),\n",
    "    ('Oslo', 'NO'),\n",
    "    ('Warsaw', 'PL'),\n",
    "    ('Lisbon', 'PT'),\n",
    "    ('Bucharest', 'RO'),\n",
    "    ('Ljubljana', 'SI'),\n",
    "    ('Bratislava', 'SK'),\n",
    "    ('Stockholm', 'SE'),\n",
    "    ('Madrid', 'ES'),\n",
    "    ('Bilbao', 'ES'),\n",
    "    ('Bern', 'CH'),\n",
    "    ('Istanbul', 'TR'),\n",
    "    ('London', 'GB')\n",
    "]\n",
    "\n",
    "# Fetch coordinates for each city\n",
    "sel_cities_coordinates = []\n",
    "for city, country in sel_cities:\n",
    "    coordinates = get_coordinates(city, country)\n",
    "    if coordinates:\n",
    "        sel_cities_coordinates.append((city, coordinates[0], coordinates[1]))\n",
    "\n",
    "# Convert to DataFrame \n",
    "sel_cities_df = pd.DataFrame(sel_cities_coordinates, columns=['city', 'latitude', 'longitude'])\n",
    "# Save DataFrame to CSV\n",
    "sel_cities_df.to_csv(coordinates_file, index=False)\n",
    "print(sel_cities_df)"
   ],
   "id": "5cf4c6969eb9f025",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d99b40f2c63ce38a",
   "metadata": {},
   "source": "## Load map in Vega-Altair"
  },
  {
   "cell_type": "code",
   "id": "806a64a2-c45a-47bd-a639-99d02badff35",
   "metadata": {},
   "source": [
    "import altair as alt\n",
    "\n",
    "#load topojson of Europe\n",
    "europe = alt.topo_feature('https://dmws.hkvservices.nl/dataportal/data.asmx/read?database=vega&key=europe', 'europe')\n",
    "\n",
    "#create the map\n",
    "base = alt.Chart(europe).mark_geoshape(\n",
    "    fill='lightgray',\n",
    "    stroke='white'\n",
    ").project(\n",
    "    'mercator'\n",
    ").properties(\n",
    "    width=1500,\n",
    "    height=1000\n",
    ")\n",
    "\n",
    "#add cities\n",
    "points = alt.Chart(cities_df).mark_circle(color='#fc9272', size=50).encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    "    tooltip=['city:N']\n",
    ")\n",
    "\n",
    "#combine basemap and cities\n",
    "map_with_cities = base + points\n",
    "map_with_cities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Plot selected cities on Vega-Altair Map",
   "id": "c1018a8ade389c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import altair as alt\n",
    "\n",
    "# Define the bounding box (extent) - northernmost and southernmost coordinates\n",
    "north_lat = 60.0\n",
    "south_lat = 34.0\n",
    "west_lon = -5.0\n",
    "east_lon = 50.0\n",
    "\n",
    "# Create a GeoJSON-like Feature for the extent\n",
    "extent_feature = {\n",
    "    \"type\": \"Feature\",\n",
    "    \"geometry\": {\n",
    "        \"type\": \"Polygon\",\n",
    "        \"coordinates\": [[\n",
    "            [east_lon, north_lat],\n",
    "            [east_lon, south_lat],\n",
    "            [west_lon, south_lat],\n",
    "            [west_lon, north_lat],\n",
    "            [east_lon, north_lat]\n",
    "        ]]\n",
    "    },\n",
    "    \"properties\": {}\n",
    "}\n",
    "\n",
    "#load topojson of Europe\n",
    "europe = alt.topo_feature('https://raw.githubusercontent.com/leakyMirror/map-of-europe/refs/heads/master/TopoJSON/europe.topojson', 'europe')\n",
    "\n",
    "#create the map\n",
    "base = alt.Chart(europe).mark_geoshape(clip=True, fill='lightgray', stroke='white').project(\n",
    "    type='mercator',\n",
    "    fit=extent_feature  # Use defined extent\n",
    ").properties(\n",
    "    width=1500,\n",
    "    height=700,\n",
    ")\n",
    "\n",
    "#add cities\n",
    "points = alt.Chart(sel_cities_df).mark_circle(color='#fc9272', size=50).encode(\n",
    "    longitude='longitude:Q',\n",
    "    latitude='latitude:Q',\n",
    "    tooltip=['city:N']\n",
    ")\n",
    "\n",
    "#combine basemap and cities\n",
    "map_with_cities = base + points\n",
    "map_with_cities"
   ],
   "id": "50ef07eaa5efd769"
  },
  {
   "cell_type": "markdown",
   "id": "59b9b1316666d9bd",
   "metadata": {},
   "source": "## Alternative - Plotly Map"
  },
  {
   "cell_type": "code",
   "id": "a7335d933e53ef02",
   "metadata": {},
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scattergeo(\n",
    "    lon = cities_df['longitude'],\n",
    "    lat = cities_df['latitude'],\n",
    "    text = cities_df['city'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        size = 8,\n",
    "        color = '#fc9272',\n",
    "        symbol = 'circle'\n",
    "    ),\n",
    "    hovertemplate=(\n",
    "        '<b>%{text}</b><br>' +\n",
    "        'Lat: %{lat:.2f} Lon: %{lon:.2f}<br>' +\n",
    "        '<extra></extra>'  #hide trace name\n",
    "    ),\n",
    "     hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        bordercolor=\"white\",\n",
    "        font_size=12,     \n",
    "        font_family=\"Open Sans\",\n",
    "        font_color=\"black\"\n",
    "        \n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title = 'Train Map of Europe',\n",
    "    geo = dict(\n",
    "        scope = 'europe',\n",
    "        projection_type = 'mercator',\n",
    "        showland = True,\n",
    "        landcolor = '#bdbdbd',\n",
    "        showcoastlines = False,\n",
    "        showlakes = False,\n",
    "        countrywidth = 0.5,\n",
    "        countrycolor = 'white',\n",
    "        lonaxis=dict(\n",
    "            range=[-11.0, 32.0]\n",
    "        ),\n",
    "        lataxis=dict(\n",
    "            range=[36.0, 60.0]\n",
    "        ),\n",
    "        resolution=50\n",
    "    ),\n",
    "    width=1000,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c0b195d102ebcd5e",
   "metadata": {},
   "source": "## Get CO2 emissions data and train travel time from [TravelCO2 API](https://travelco2.com/documentation)"
  },
  {
   "cell_type": "code",
   "id": "f5342f338ac18426",
   "metadata": {},
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://travelco2.com/api/v1/simpletrips\"\n",
    "\n",
    "payload = {\n",
    "    \"from\": \"Berlin, Germany\",\n",
    "    \"to\": \"Amsterdam, Netherlands\",\n",
    "    \"ways\": 1,\n",
    "    \"people\": 1,\n",
    "    \"language\": \"en\",\n",
    "    \"title\": \"Comparing flying and public transport from Berlin to Amsterdam.\",\n",
    "    \"transport_types\": [\"flying\", \"public-transport\"]\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer YOUR_APIKEY\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.json())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf52b7abfa2a6f21",
   "metadata": {},
   "source": [
    "#Version with manual skipping and stopping\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import keyboard\n",
    "import time\n",
    "\n",
    "# File paths\n",
    "log_directory = 'api_logs'\n",
    "coordinates_file = 'data/coordinates.csv'\n",
    "\n",
    "# Generate a unique log file name based on the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(log_directory, f'api_responses_{timestamp}.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Read cities from the CSV file\n",
    "coordinates_df = pd.read_csv(coordinates_file)\n",
    "cities = coordinates_df['city'].tolist()\n",
    "\n",
    "# Function to make API call and process data\n",
    "def get_trip_data(start, end):\n",
    "    url = \"https://travelco2.com/api/v1/simpletrips\"\n",
    "    payload = {\n",
    "        \"from\": start,\n",
    "        \"to\": end,\n",
    "        \"ways\": 1,\n",
    "        \"people\": 1,\n",
    "        \"language\": \"en\",\n",
    "        \"title\": f\"Comparing flying and public transport from {start} to {end}.\",\n",
    "        \"transport_types\": [\"flying\", \"public-transport\"]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer YOUR_APIKEY\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Log the response to a file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "        f.write('\\n')\n",
    "\n",
    "# Function to handle user actions\n",
    "def handle_action(request_counter, start, end):\n",
    "    print(f\"API request {request_counter}: {start} to {end}.\")\n",
    "    print(\"Press Shift to Skip, Enter to Proceed, Esc to Stop\")\n",
    "\n",
    "    while True:\n",
    "        if keyboard.is_pressed('esc'):\n",
    "            print(\"Stopping script...\")\n",
    "            sys.exit()\n",
    "\n",
    "        if keyboard.is_pressed('shift'):\n",
    "            time.sleep(0.3)  # Wait to avoid multiple detections\n",
    "            print(\"Skipping request...\")\n",
    "            return 'skip'\n",
    "\n",
    "        if keyboard.is_pressed('enter'):\n",
    "            time.sleep(0.3)  # Wait to avoid multiple detections\n",
    "            print(\"Proceeding with request...\")\n",
    "            return 'proceed'\n",
    "\n",
    "        time.sleep(0.1)  # Short sleep to prevent high CPU usage\n",
    "\n",
    "# Loop through the cities and make API requests\n",
    "processed_pairs = set()\n",
    "request_counter = 0\n",
    "for i in range(len(cities)):\n",
    "    for j in range(i + 1, len(cities)):\n",
    "        if (cities[i], cities[j]) not in processed_pairs and (cities[j], cities[i]) not in processed_pairs:\n",
    "            action = handle_action(request_counter + 1, cities[i], cities[j])\n",
    "            if action == 'proceed':\n",
    "                get_trip_data(cities[i], cities[j])\n",
    "                request_counter += 1\n",
    "                print(f\"API request {request_counter} completed: {cities[i]} to {cities[j]}\")\n",
    "                processed_pairs.add((cities[i], cities[j]))\n",
    "            elif action == 'skip':\n",
    "                processed_pairs.add((cities[i], cities[j]))\n",
    "        else:\n",
    "            print(f\"Request from {cities[i]} to {cities[j]} has already been processed.\")\n",
    "            \n",
    "print(\"All API requests have been completed and logs have been stored.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "168579c674b4639f",
   "metadata": {},
   "source": [
    "#Version with skipping defined number of requests\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# File paths\n",
    "log_directory = 'api_logs'\n",
    "coordinates_file = 'data/coordinates.csv'\n",
    "\n",
    "# Generate a unique log file name based on the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(log_directory, f'api_responses_{timestamp}.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Read cities from the CSV file\n",
    "coordinates_df = pd.read_csv(coordinates_file)\n",
    "cities = coordinates_df['city'].tolist()\n",
    "\n",
    "# Function to make API call and process data\n",
    "def get_trip_data(start, end):\n",
    "    url = \"https://travelco2.com/api/v1/simpletrips\"\n",
    "    payload = {\n",
    "        \"from\": start,\n",
    "        \"to\": end,\n",
    "        \"ways\": 1,\n",
    "        \"people\": 1,\n",
    "        \"language\": \"en\",\n",
    "        \"title\": f\"Comparing flying and public transport from {start} to {end}.\",\n",
    "        \"transport_types\": [\"flying\", \"public-transport\"]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer YOUR_APIKEY\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Log the response to a file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "        f.write('\\n')\n",
    "        \n",
    "    # Check if the response was successful\n",
    "    if not data.get(\"success\", True):\n",
    "        print(f\"API request failed for {start} to {end}. Stopping script.\")\n",
    "        sys.exit()\n",
    "\n",
    "# Loop through the cities and make API requests\n",
    "processed_pairs = set()\n",
    "request_counter = 0  # Counter to see the number of requests\n",
    "skip_count = 361     # Number of requests to skip initially\n",
    "\n",
    "for i in range(len(cities)):\n",
    "    for j in range(i + 1, len(cities)):\n",
    "        if (cities[i], cities[j]) not in processed_pairs and (cities[j], cities[i]) not in processed_pairs:\n",
    "            if request_counter < skip_count:\n",
    "                print(f\"Skipping API request {request_counter + 1}: {cities[i]} to {cities[j]}\")\n",
    "                request_counter += 1\n",
    "                processed_pairs.add((cities[i], cities[j]))\n",
    "                continue\n",
    "            \n",
    "            print(f\"Proceeding with API request {request_counter + 1}: {cities[i]} to {cities[j]}\")\n",
    "            get_trip_data(cities[i], cities[j])\n",
    "            request_counter += 1\n",
    "            print(f\"API request {request_counter} completed: {cities[i]} to {cities[j]}\")\n",
    "            processed_pairs.add((cities[i], cities[j]))\n",
    "            \n",
    "            # Adding a 0.5-second pause between requests\n",
    "            time.sleep(0.5)\n",
    "        else:\n",
    "            print(f\"Request from {cities[i]} to {cities[j]} has already been processed.\")\n",
    "\n",
    "print(\"All API requests have been completed and logs have been stored.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#version for getting missing/extra pairs of cities\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import time  # Importing time module to add a pause\n",
    "\n",
    "# File paths\n",
    "log_directory = 'api_logs'\n",
    "\n",
    "# Generate a unique log file name based on the current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(log_directory, f'api_responses_{timestamp}.log')\n",
    "\n",
    "# Ensure the log directory exists\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "\n",
    "# Predefined list of city pairs for API requests\n",
    "city_pairs = [\n",
    "    (\"Luxembourg City\", \"Vienna\"),\n",
    "    (\"Luxembourg City\", \"Brussels\"),\n",
    "    (\"Luxembourg City\", \"Sofia\"),\n",
    "    (\"Luxembourg City\", \"Zagreb\"),\n",
    "    (\"Luxembourg City\", \"Prague\"),\n",
    "    (\"Luxembourg City\", \"Copenhagen\"),\n",
    "    (\"Luxembourg City\", \"Tallinn\"),\n",
    "    (\"Luxembourg City\", \"Paris\"),\n",
    "    (\"Luxembourg City\", \"Berlin\"),\n",
    "    (\"Luxembourg City\", \"Dresden\"),\n",
    "    (\"Luxembourg City\", \"Munich\"),\n",
    "    (\"Luxembourg City\", \"Budapest\"),\n",
    "    (\"Luxembourg City\", \"Rome\"),\n",
    "    (\"Luxembourg City\", \"Riga\"),\n",
    "    (\"Luxembourg City\", \"Vilnius\"),\n",
    "    (\"Luxembourg City\", \"Amsterdam\"),\n",
    "    (\"Luxembourg City\", \"Oslo\"),\n",
    "    (\"Luxembourg City\", \"Warsaw\"),\n",
    "    (\"Luxembourg City\", \"Lisbon\"),\n",
    "    (\"Luxembourg City\", \"Bucharest\"),\n",
    "    (\"Luxembourg City\", \"Ljubljana\"),\n",
    "    (\"Luxembourg City\", \"Bratislava\"),\n",
    "    (\"Luxembourg City\", \"Stockholm\"),\n",
    "    (\"Luxembourg City\", \"Madrid\"),\n",
    "    (\"Luxembourg City\", \"Bilbao\"),\n",
    "    (\"Luxembourg City\", \"Bern\"),\n",
    "    (\"Luxembourg City\", \"Istanbul\"),\n",
    "    (\"Luxembourg City\", \"London\"),\n",
    "    (\"London\", \"Tallinn\"),\n",
    "    (\"Sofia\", \"Ljubljana\"),\n",
    "    (\"Munich\", \"Prague\"),\n",
    "    (\"Amsterdam\", \"Sofia\"),\n",
    "    (\"Dresden\", \"Frankfurt\"),\n",
    "    (\"Frankfurt\", \"Paris\"),\n",
    "    (\"Istanbul\", \"Plovdiv\"),\n",
    "    (\"Plovdiv\", \"Sofia\")\n",
    "]\n",
    "\n",
    "# Function to make API call and process data\n",
    "def get_trip_data(start, end):\n",
    "    url = \"https://travelco2.com/api/v1/simpletrips\"\n",
    "    payload = {\n",
    "        \"from\": start,\n",
    "        \"to\": end,\n",
    "        \"ways\": 1,\n",
    "        \"people\": 1,\n",
    "        \"language\": \"en\",\n",
    "        \"title\": f\"Comparing flying and public transport from {start} to {end}.\",\n",
    "        \"transport_types\": [\"flying\", \"public-transport\"]\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer YOUR_APIKEY\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Log the response to a file\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(json.dumps(data, indent=4))\n",
    "        f.write('\\n')\n",
    "    \n",
    "    # Check if the response was successful\n",
    "    if not data.get(\"success\", True):\n",
    "        print(f\"API request failed for {start} to {end}. Stopping script.\")\n",
    "        sys.exit()\n",
    "\n",
    "# Loop through the city pairs and make API requests\n",
    "request_counter = 0  # Counter to see the number of requests\n",
    "\n",
    "for start, end in city_pairs:\n",
    "    print(f\"Proceeding with API request {request_counter + 1}: {start} to {end}\")\n",
    "    get_trip_data(start, end)\n",
    "    request_counter += 1\n",
    "    print(f\"API request {request_counter} completed: {start} to {end}\")\n",
    "\n",
    "    # Adding a 0.5-second pause between requests\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"All API requests have been completed and logs have been stored.\")"
   ],
   "id": "d30456944aab9c5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e18f161279df84c8",
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "log_directory = 'api_logs'\n",
    "# log_file = 'api_logs/api_responses_extra.log'\n",
    "csv_file = 'data/trips_data_1508_edit1.csv'\n",
    "log_files = [os.path.join(log_directory, f) for f in os.listdir(log_directory) if f.endswith('.log')]\n",
    "\n",
    "# Function to extract data from JSON text\n",
    "def extract_data_from_text(text):\n",
    "    try:\n",
    "        title_match = re.search(r'\"title\": \"Comparing flying and public transport from (.+?) to (.+?)\\.\"', text)\n",
    "        if not title_match:\n",
    "            raise ValueError(\"Title format does not match\")\n",
    "        city1 = title_match.group(1)\n",
    "        city2 = title_match.group(2).rstrip(\".\")\n",
    "        \n",
    "        json_data = json.loads(text)\n",
    "        trips = json_data.get(\"trips\", [])\n",
    "        \n",
    "        if not trips:\n",
    "            raise ValueError(\"No trip data found\")\n",
    "        # trips = json_data[\"trips\"]\n",
    "        # if len(trips) < 2:\n",
    "        #     raise ValueError(\"Not enough trips data\")\n",
    "\n",
    "        co2_plane = round(trips[0][\"co2e\"], 2)\n",
    "        co2_train = round(trips[1][\"co2e\"], 2)\n",
    "\n",
    "        duration_seconds = sum(step[\"transport\"][\"duration\"] for step in trips[1][\"steps\"] if step.get(\"transport\") and step[\"transport\"].get(\"duration\"))\n",
    "        \n",
    "        # Converting duration to h:mm format\n",
    "        hours, remainder = divmod(duration_seconds, 3600)\n",
    "        minutes = remainder // 60\n",
    "        duration_train = f\"{int(hours)}:{int(minutes):02d}\"\n",
    "        \n",
    "        # train_less_co2_to_plane = round((co2_plane - co2_train) / co2_plane * 100, 2)\n",
    "        # plane_more_co2_to_train = round((co2_plane / co2_train), 2)\n",
    "\n",
    "        return city1, city2, duration_train, co2_train, co2_plane #train_less_co2_to_plane, plane_more_co2_to_train\n",
    "    except (IndexError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error extracting data from text: {e}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Check if the CSV file exists and load it, otherwise initialize an empty DataFrame\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"City_1\", \"City_2\", \"Duration_train\", \"Train_CO2_kg\", \"Plane_CO2_kg\"])\n",
    "\n",
    "# Process each log file\n",
    "for log_file in log_files:\n",
    "    with open(log_file, 'r') as f:\n",
    "        content = f.read()\n",
    "        entries = content.split('}\\n{')\n",
    "        \n",
    "        # Adjust entries to ensure they are valid JSON strings\n",
    "        entries = [entry + '}' if not entry.endswith('}') else entry for entry in entries]\n",
    "        entries = ['{' + entry if not entry.startswith('{') else entry for entry in entries]\n",
    "    \n",
    "        for entry in entries:\n",
    "            if entry.strip():  # Ensure it's not an empty line\n",
    "                city1, city2, duration_train, co2_train, co2_plane = extract_data_from_text(entry)\n",
    "                if city1 and city2:\n",
    "                    df = df.append({\n",
    "                        \"City_1\": city1,\n",
    "                        \"City_2\": city2,\n",
    "                        \"Duration_train\": duration_train,\n",
    "                        \"Train_CO2_kg\": co2_train,\n",
    "                        \"Plane_CO2_kg\": co2_plane,\n",
    "                    }, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Data has been extracted and saved successfully.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#one log file, when no train data\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "log_file = 'api_logs/api_responses_extra.log'\n",
    "csv_file = 'data/trips_data_1508_edit1.csv'\n",
    "\n",
    "# Function to extract data from JSON text\n",
    "def extract_data_from_text(text):\n",
    "    try:\n",
    "        title_match = re.search(r'\"title\": \"Comparing flying and public transport from (.+?) to (.+?)\\.\"', text)\n",
    "        if not title_match:\n",
    "            raise ValueError(\"Title format does not match\")\n",
    "        city1 = title_match.group(1)\n",
    "        city2 = title_match.group(2).rstrip(\".\")\n",
    "        \n",
    "        json_data = json.loads(text)\n",
    "        trips = json_data.get(\"trips\", [])\n",
    "        \n",
    "        co2_plane, co2_train, duration_train = None, None, None\n",
    "\n",
    "        if trips:\n",
    "            co2_plane = round(trips[0][\"co2e\"], 2)\n",
    "\n",
    "            if len(trips) > 1 and trips[1].get(\"steps\"):\n",
    "                co2_train = round(trips[1][\"co2e\"], 2)\n",
    "                duration_seconds = sum(\n",
    "                    step[\"transport\"][\"duration\"] for step in trips[1][\"steps\"]\n",
    "                    if step.get(\"transport\") and step[\"transport\"].get(\"duration\")\n",
    "                )\n",
    "\n",
    "                # Convert duration to h:mm format\n",
    "                hours, remainder = divmod(duration_seconds, 3600)\n",
    "                minutes = remainder // 60\n",
    "                duration_train = f\"{int(hours)}:{int(minutes):02d}\"\n",
    "\n",
    "        return city1, city2, duration_train, co2_train, co2_plane\n",
    "    except (IndexError, AttributeError, ValueError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error extracting data from text: {e}\")\n",
    "        return None, None, None, None, None\n",
    "        \n",
    "# Check if the CSV file exists and load it, otherwise initialize an empty DataFrame\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"City_1\", \"City_2\", \"Duration_train\", \"Train_CO2_kg\", \"Plane_CO2_kg\"])\n",
    "\n",
    "# Process each log file\n",
    "with open(log_file, 'r') as f:\n",
    "    content = f.read()\n",
    "    entries = content.split('}\\n{')\n",
    "    \n",
    "    # Adjust entries to ensure they are valid JSON strings\n",
    "    entries = [entry + '}' if not entry.endswith('}') else entry for entry in entries]\n",
    "    entries = ['{' + entry if not entry.startswith('{') else entry for entry in entries]\n",
    "\n",
    "    for entry in entries:\n",
    "        if entry.strip():  # Ensure it's not an empty line\n",
    "            city1, city2, duration_train, co2_train, co2_plane = extract_data_from_text(entry)\n",
    "            if city1 and city2:\n",
    "                df = df.append({\n",
    "                    \"City_1\": city1,\n",
    "                    \"City_2\": city2,\n",
    "                    \"Duration_train\": duration_train,\n",
    "                    \"Train_CO2_kg\": co2_train,\n",
    "                    \"Plane_CO2_kg\": co2_plane,\n",
    "                }, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Data has been extracted and saved successfully.\")"
   ],
   "id": "4213781fa0b85d8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#for one log file, CO2 emissions from plane only\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File path\n",
    "log_file = 'api_logs/misc/found.txt'\n",
    "csv_file = 'data/trips_data_1508_edit1.csv'\n",
    "\n",
    "# Function to extract Plane_CO2_kg from JSON text\n",
    "def extract_co2_from_text(text):\n",
    "    try:\n",
    "        title_match = re.search(r'\"title\": \"Comparing flying and public transport from (.+?) to (.+?)\\.\"', text)\n",
    "        if not title_match:\n",
    "            raise ValueError(\"Title format does not match\")\n",
    "        city1 = title_match.group(1)\n",
    "        city2 = title_match.group(2).rstrip(\".\")\n",
    "        \n",
    "        json_data = json.loads(text)\n",
    "        trips = json_data.get(\"trips\", [])\n",
    "        if not trips:\n",
    "            raise ValueError(\"No trip data found\")\n",
    "        \n",
    "        # Extracting CO2 emissions for the plane (first trip in the JSON structure)\n",
    "        co2_plane = round(trips[0].get(\"co2e\", 0), 2)\n",
    "        \n",
    "        return city1, city2, co2_plane\n",
    "    except (json.JSONDecodeError, IndexError, ValueError) as e:\n",
    "        print(f\"Error extracting data from text: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Load existing CSV file\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\n",
    "        \"City_1\",\n",
    "        \"City_2\",\n",
    "        \"Duration_train\",\n",
    "        \"Train_CO2_kg\",\n",
    "        \"Plane_CO2_kg\"\n",
    "    ])\n",
    "# Process the log file\n",
    "with open(log_file, 'r') as f:\n",
    "    content = f.read()\n",
    "    entries = content.split('}\\n{')\n",
    "    \n",
    "    # Adjust entries to ensure they are valid JSON strings\n",
    "    entries = [entry + '}' if not entry.endswith('}') else entry for entry in entries]\n",
    "    entries = ['{' + entry if not entry.startswith('{') else entry for entry in entries]\n",
    "\n",
    "    for entry in entries:\n",
    "        if entry.strip():  # Ensure it's not an empty line\n",
    "            city1, city2, co2_plane = extract_co2_from_text(entry)\n",
    "            if city1 and city2:\n",
    "                df = df.append({\n",
    "                    \"City_1\": city1,\n",
    "                    \"City_2\": city2,\n",
    "                    \"Duration_train\": \"\",\n",
    "                    \"Train_CO2_kg\": \"\",\n",
    "                    \"Plane_CO2_kg\": co2_plane\n",
    "                }, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Data has been extracted and appended to the CSV file successfully.\")"
   ],
   "id": "140c2ef7313d8cc4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#calculate missing values for train routes based on existing data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# File path\n",
    "csv_file = 'data/trips_data_1508_edit1.csv'\n",
    "\n",
    "# Function to convert duration in h:mm format to total minutes\n",
    "def duration_to_minutes(duration_str):\n",
    "    if pd.isna(duration_str) or duration_str == \"\":\n",
    "        return None\n",
    "    hours, minutes = map(int, duration_str.split(':'))\n",
    "    return hours * 60 + minutes\n",
    "\n",
    "# Function to convert total minutes to h:mm format\n",
    "def minutes_to_duration(minutes):\n",
    "    if minutes is None:\n",
    "        return \"\"\n",
    "    hours = minutes // 60\n",
    "    minutes = minutes % 60\n",
    "    return f\"{hours}:{minutes:02d}\"\n",
    "\n",
    "# Load existing CSV file\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"The file {csv_file} does not exist.\")\n",
    "\n",
    "# Define the calculation logic for routes\n",
    "routes_map = {\n",
    "    \"Luxembourg City,Tallinn\": [(\"Luxembourg City\", \"Berlin\"), (\"Berlin\", \"Tallinn\")],\n",
    "    \"Dresden,Paris\": [(\"Dresden\", \"Frankfurt\"), (\"Frankfurt\", \"Paris\")],\n",
    "    \"Istanbul,Sofia\": [(\"Istanbul\", \"Plovdiv\"), (\"Plovdiv\", \"Sofia\")],\n",
    "    \"Bern,Lisbon\": [(\"Bern\", \"Madrid\"), (\"Madrid\", \"Lisbon\")],\n",
    "    \"Bern,Tallinn\": [(\"Bern\", \"Vilnius\"), (\"Vilnius\", \"Tallinn\")],\n",
    "    \"Bilbao,Oslo\": [(\"Bilbao\", \"Brussels\"), (\"Brussels\", \"Oslo\")],\n",
    "    \"Bilbao,Riga\": [(\"Bilbao\", \"Vilnius\"), (\"Vilnius\", \"Riga\")],\n",
    "    \"Bilbao,Rome\": [(\"Bilbao\", \"Paris\"), (\"Paris\", \"Rome\")],\n",
    "    \"Bilbao,Stockholm\": [(\"Bilbao\", \"Paris\"), (\"Paris\", \"Stockholm\")],\n",
    "    \"Bilbao,Tallinn\": [(\"Bilbao\", \"Vilnius\"), (\"Vilnius\", \"Tallinn\")],\n",
    "    \"Brussels,Riga\": [(\"Brussels\", \"Warsaw\"), (\"Warsaw\", \"Vilnius\"), (\"Vilnius\", \"Riga\")],\n",
    "    \"Dresden,Istanbul\": [(\"Dresden\", \"Prague\"), (\"Prague\", \"Istanbul\")],\n",
    "    \"Istanbul,Lisbon\": [(\"Istanbul\", \"Madrid\"), (\"Madrid\", \"Lisbon\")],\n",
    "    \"Istanbul,Riga\": [(\"Istanbul\", \"Bucharest\"), (\"Bucharest\", \"Riga\")],\n",
    "    \"Istanbul,Tallinn\": [(\"Istanbul\", \"Bucharest\"), (\"Bucharest\", \"Tallinn\")],\n",
    "    \"Istanbul,Vilnius\": [(\"Istanbul\", \"Bucharest\"), (\"Bucharest\", \"Vilnius\")],\n",
    "    \"Istanbul,Warsaw\": [(\"Istanbul\", \"Bucharest\"), (\"Bucharest\", \"Warsaw\")],\n",
    "    \"Lisbon,Oslo\": [(\"Lisbon\", \"Madrid\"), (\"Madrid\", \"Oslo\")],\n",
    "    \"London,Vilnius\": [(\"London\", \"Berlin\"), (\"Berlin\", \"Vilnius\")],\n",
    "    \"London,Warsaw\": [(\"London\", \"Berlin\"), (\"Berlin\", \"Warsaw\")],\n",
    "    \"Oslo,Sofia\": [(\"Oslo\", \"Berlin\"), (\"Berlin\", \"Sofia\")],\n",
    "    \"Rome,Sofia\": [(\"Rome\", \"Bucharest\"), (\"Bucharest\", \"Sofia\")],\n",
    "    \"Tallinn,Warsaw\": [(\"Tallinn\", \"Vilnius\"), (\"Vilnius\", \"Warsaw\")],\n",
    "    \"Tallinn,Vienna\": [(\"Tallinn\", \"Vilnius\"), (\"Vilnius\", \"Warsaw\"), (\"Warsaw\", \"Vienna\")],\n",
    "    \"Tallinn,Zagreb\": [(\"Tallinn\", \"Vilnius\"), (\"Vilnius\", \"Warsaw\"), (\"Warsaw\", \"Zagreb\")]\n",
    "}\n",
    "\n",
    "# Function to calculate missing values\n",
    "def calculate_missing_values(row):\n",
    "    route = f\"{row['City_1']},{row['City_2']}\"\n",
    "    if route in routes_map:\n",
    "        sub_routes = routes_map[route]\n",
    "        total_duration_minutes = 0\n",
    "        total_co2_kg = 0.0\n",
    "        for city1, city2 in sub_routes:\n",
    "            sub_route = df[((df['City_1'] == city1) & (df['City_2'] == city2)) | \n",
    "                           ((df['City_1'] == city2) & (df['City_2'] == city1))]\n",
    "            if not sub_route.empty:\n",
    "                duration = duration_to_minutes(sub_route.iloc[0]['Duration_train'])\n",
    "                co2_kg = sub_route.iloc[0]['Train_CO2_kg']\n",
    "                total_duration_minutes += duration if duration is not None else 0\n",
    "                total_co2_kg += co2_kg if co2_kg is not None else 0\n",
    "\n",
    "        # Update the DataFrame\n",
    "        df.at[idx, 'Duration_train'] = minutes_to_duration(total_duration_minutes)\n",
    "        df.at[idx, 'Train_CO2_kg'] = round(total_co2_kg, 2)\n",
    "\n",
    "# Apply the function to calculate missing values\n",
    "for idx, row in df.iterrows():\n",
    "    if pd.isna(row['Duration_train']) or row['Duration_train'] == \"\" or pd.isna(row['Train_CO2_kg']) or row['Train_CO2_kg'] == \"\":\n",
    "        calculate_missing_values(row)\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Missing values have been calculated and the CSV file has been updated successfully.\")"
   ],
   "id": "a173bfc7955cbc4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Get flight duration from [AeroDataBox API](https://doc.aerodatabox.com/index.html?urls.primaryName=API.Market)",
   "id": "65dd3715f04ca7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#add airport codes\n",
    "import pandas as pd\n",
    "\n",
    "# File path\n",
    "csv_file = 'data/trips_data.csv'\n",
    "\n",
    "# Dictionary mapping cities to their airport codes\n",
    "airport_codes = {\n",
    "    \"Amsterdam\": \"AMS\",\n",
    "    \"Berlin\": \"BER\",\n",
    "    \"Bern\": \"BRN\",\n",
    "    \"Bilbao\": \"BIO\",\n",
    "    \"Bratislava\": \"BTS\",\n",
    "    \"Brussels\": \"BRU\",\n",
    "    \"Bucharest\": \"OTP\",\n",
    "    \"Budapest\": \"BUD\",\n",
    "    \"Copenhagen\": \"CPH\",\n",
    "    \"Dresden\": \"DRS\",\n",
    "    \"Istanbul\": \"IST\",\n",
    "    \"Lisbon\": \"LIS\",\n",
    "    \"Ljubljana\": \"LJU\",\n",
    "    \"London\": \"LHR\",\n",
    "    \"Luxembourg City\": \"LUX\",\n",
    "    \"Madrid\": \"MAD\",\n",
    "    \"Munich\": \"MUC\",\n",
    "    \"Oslo\": \"OSL\",\n",
    "    \"Paris\": \"CDG\",\n",
    "    \"Prague\": \"PRG\",\n",
    "    \"Riga\": \"RIX\",\n",
    "    \"Rome\": \"FCO\",\n",
    "    \"Sofia\": \"SOF\",\n",
    "    \"Stockholm\": \"ARN\",\n",
    "    \"Tallinn\": \"TLL\",\n",
    "    \"Vienna\": \"VIE\",\n",
    "    \"Vilnius\": \"VNO\",\n",
    "    \"Warsaw\": \"WAW\",\n",
    "    \"Zagreb\": \"ZAG\"\n",
    "}\n",
    "\n",
    "# Load existing CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to get airport code for a city\n",
    "def get_airport_code(city):\n",
    "    return airport_codes.get(city, \"\")\n",
    "\n",
    "# Create the AIR_1 and AIR_2 columns\n",
    "df['AIR_1'] = df['City_1'].apply(get_airport_code)\n",
    "df['AIR_2'] = df['City_2'].apply(get_airport_code)\n",
    "\n",
    "# Reorder the columns to insert AIR_1 and AIR_2 after City_2\n",
    "columns_order = ['ID', 'City_1', 'City_2', 'AIR_1', 'AIR_2', 'Duration_train', 'Train_CO2_kg', 'Plane_CO2_kg','Duration_plane']\n",
    "df = df[columns_order]\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"AIR_1 and AIR_2 columns have been added and the CSV file has been updated successfully.\")"
   ],
   "id": "223a4106de3770fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Add flight duration data from API\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Define the API key and the base URL\n",
    "API_KEY = 'Your API key' #https://api.market/store/aedbx/aerodatabox\n",
    "\n",
    "# Create the flights_API directory if it doesn't exist\n",
    "log_dir = 'flights_API'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the merged log file\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "merged_log_file = os.path.join(log_dir, f'API_log_{timestamp}.txt')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/trips_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to fetch flight duration from the API\n",
    "def get_flight_duration(airport_from, airport_to):\n",
    "    url = f'https://api.magicapi.dev/api/v1/aedbx/aerodatabox/airports/Iata/{airport_from}/distance-time/{airport_to}?flightTimeModel=ML01'\n",
    "    headers = {\n",
    "        'accept': 'application/json',\n",
    "        'x-magicapi-key': API_KEY\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Print the raw response for debugging\n",
    "    print(f'Response from {url}:\\n{response.text}')\n",
    "\n",
    "    try:\n",
    "        response_data = response.json()\n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Log the response with a timestamp\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    with open(merged_log_file, 'a', encoding='utf-8') as log:\n",
    "        log.write(f'Time: {timestamp}\\n')\n",
    "        log.write(f'From: {airport_from} To: {airport_to}\\n')\n",
    "        log.write(f'URL: {url}\\n')\n",
    "        log.write(f'Response: {response.text}\\n')\n",
    "        log.write('\\n')\n",
    "\n",
    "    # Extract the flight duration in \"hh:mm\" format\n",
    "    approx_flight_time = response_data.get('approxFlightTime', None)\n",
    "    \n",
    "    if approx_flight_time:\n",
    "        # Convert \"hh:mm:ss\" to \"hh:mm\"\n",
    "        hh_mm = approx_flight_time[:5]\n",
    "        return hh_mm\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Apply the API call with a 1-second interval between requests\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Duration_plane'] = get_flight_duration(row['AIR_1'], row['AIR_2'])\n",
    "    time.sleep(1) \n",
    "\n",
    "# Save the updated CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Flight durations have been added to the CSV file.\")"
   ],
   "id": "3b7aa74a2b5878cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Define the directory and log file paths\n",
    "log_dir = 'flights_API'\n",
    "merged_log_file = os.path.join(log_dir, 'API_log_merged.txt')\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/trips_data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to parse the log file and extract flight durations\n",
    "def parse_log_file():\n",
    "    flight_durations = {}\n",
    "\n",
    "    with open(merged_log_file, 'r', encoding='utf-8') as log:\n",
    "        log_content = log.read()\n",
    "        \n",
    "        # Find all relevant log entries using regex\n",
    "        entries = re.findall(r'Time: (.*?)\\nFrom: (.*?) To: (.*?)\\nURL: .*?\\nResponse: (.*?)\\n', log_content, re.DOTALL)\n",
    "        \n",
    "        for entry in entries:\n",
    "            timestamp, airport_from, airport_to, response = entry\n",
    "            \n",
    "            # Extract the flight duration from the response using regex\n",
    "            duration_match = re.search(r'\"approxFlightTime\":\"(\\d{2}:\\d{2}:\\d{2})\"', response)\n",
    "            \n",
    "            if duration_match:\n",
    "                hh_mm = duration_match.group(1)[:5]  # Convert \"hh:mm:ss\" to \"hh:mm\"\n",
    "                flight_durations[(airport_from, airport_to)] = hh_mm\n",
    "\n",
    "    return flight_durations\n",
    "\n",
    "# Parse the log file to get the flight durations\n",
    "flight_durations = parse_log_file()\n",
    "\n",
    "# Update the DataFrame with the parsed durations\n",
    "for index, row in df.iterrows():\n",
    "    airport_pair = (row['AIR_1'], row['AIR_2'])\n",
    "    if airport_pair in flight_durations:\n",
    "        df.at[index, 'Duration_plane'] = flight_durations[airport_pair]\n",
    "\n",
    "# Save the updated CSV file\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Flight durations have been updated in the CSV file from the log.\")"
   ],
   "id": "db69a65f2b19b3b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# File path\n",
    "csv_file = 'data/trips_data.csv'\n",
    "\n",
    "# Load existing CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Function to add hours to a time string\n",
    "def add_hours_to_duration(duration_str, hours_to_add):\n",
    "    try:\n",
    "        # Parse the duration string\n",
    "        time_obj = datetime.strptime(duration_str, '%H:%M')\n",
    "        # Add the specified hours\n",
    "        new_time = time_obj + timedelta(hours=hours_to_add)\n",
    "        # Format the new time as H:MM\n",
    "        return new_time.strftime('%H:%M')\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing duration '{duration_str}': {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Add 3 hours to each duration in Duration_plane\n",
    "df['Duration_plane_total'] = df['Duration_plane'].apply(lambda x: add_hours_to_duration(x, 3))\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Duration_plane_total column has been added and the CSV file has been updated successfully.\")"
   ],
   "id": "6b64fd30675d9b87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating train routes (lines and points Geojson files) from coordinates of the intermediate stops in the TravelCO2 API request logs",
   "id": "b9e8ab6e63c02fd7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#version for extracting intermediate stops data\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# File paths\n",
    "log_file = 'api_logs/api_responses_extra.log'\n",
    "csv_file = 'data/plovdiv_intermediate_stops_data.csv'\n",
    "\n",
    "# Function to extract intermediate stop data from JSON text\n",
    "def extract_stops_from_text(text):\n",
    "    try:\n",
    "        title_match = re.search(r'\"title\": \"Comparing flying and public transport from (.+?) to (.+?)\\.\"', text)\n",
    "        if not title_match:\n",
    "            raise ValueError(\"Title format does not match\")\n",
    "        city1 = title_match.group(1)\n",
    "        city2 = title_match.group(2).rstrip(\".\")\n",
    "        \n",
    "        json_data = json.loads(text)\n",
    "        trips = json_data.get(\"trips\", [])\n",
    "        \n",
    "        # Ensure there are at least two trips\n",
    "        if len(trips) < 2:\n",
    "            raise ValueError(\"Not enough trips data\")\n",
    "        \n",
    "        steps = trips[1].get(\"steps\", [])\n",
    "        \n",
    "        stops = []\n",
    "        for step in steps:\n",
    "            if \"location\" in step:\n",
    "                location = step[\"location\"]\n",
    "                stops.append({\n",
    "                    \"placename\": location.get(\"placename\", \"\"),\n",
    "                    \"latitude\": location.get(\"latitude\", \"\"),\n",
    "                    \"longitude\": location.get(\"longitude\", \"\")\n",
    "                })\n",
    "\n",
    "        return city1, city2, stops\n",
    "    except (IndexError, AttributeError, ValueError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error extracting stops data from text: {e}\")\n",
    "        return None, None, []\n",
    "\n",
    "# Check if the CSV file exists and load it, otherwise initialize an empty DataFrame\n",
    "if os.path.exists(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "else:\n",
    "    df = pd.DataFrame(columns=[\"City_1\", \"City_2\"])\n",
    "\n",
    "# Process each log file\n",
    "with open(log_file, 'r') as f:\n",
    "    content = f.read()\n",
    "    entries = content.split('}\\n{')\n",
    "\n",
    "    # Adjust entries to ensure they are valid JSON strings\n",
    "    entries = [entry + '}' if not entry.endswith('}') else entry for entry in entries]\n",
    "    entries = ['{' + entry if not entry.startswith('{') else entry for entry in entries]\n",
    "\n",
    "    for entry in entries:\n",
    "        if entry.strip():  # Ensure it's not an empty line\n",
    "            city1, city2, stops = extract_stops_from_text(entry)\n",
    "            if city1 and city2:\n",
    "                row_data = {\"City_1\": city1, \"City_2\": city2}\n",
    "                \n",
    "                for i, stop in enumerate(stops, start=1):\n",
    "                    row_data[f\"Stop_{i}\"] = stop[\"placename\"]\n",
    "                    row_data[f\"Stop_{i}_lat\"] = stop[\"latitude\"]\n",
    "                    row_data[f\"Stop_{i}_lon\"] = stop[\"longitude\"]\n",
    "                \n",
    "                df = df.append(row_data, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"Intermediate stops data has been extracted and saved successfully.\")"
   ],
   "id": "d2a12820d5ed40a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#version for extracting intermediate stops data from multiple log files based on existing csv file\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "trips_csv_file = 'data/trips_data.csv'\n",
    "logs_directory = 'api_logs'\n",
    "output_csv_file = 'data/trips_data_with_stops.csv'\n",
    "\n",
    "# Load the trips data CSV file\n",
    "trips_df = pd.read_csv(trips_csv_file)\n",
    "\n",
    "# Function to extract stop data from JSON text\n",
    "def extract_stops_from_text(text):\n",
    "    try:\n",
    "        # Match the route based on title\n",
    "        title_match = re.search(r'\"title\": \"Comparing flying and public transport from (.+?) to (.+?)\\.\"', text)\n",
    "        if not title_match:\n",
    "            raise ValueError(\"Title format does not match\")\n",
    "        city1 = title_match.group(1)\n",
    "        city2 = title_match.group(2).rstrip(\".\")\n",
    "\n",
    "        json_data = json.loads(text)\n",
    "        trips = json_data.get(\"trips\", [])\n",
    "        \n",
    "        if len(trips) < 2 or not trips[1].get(\"steps\"):\n",
    "            return city1, city2, [\"MANUAL\"]\n",
    "\n",
    "        stops = []\n",
    "        for step in trips[1][\"steps\"]:\n",
    "            if \"location\" in step:\n",
    "                placename = step[\"location\"].get(\"placename\", \"Unknown\")\n",
    "                latitude = round(step[\"location\"].get(\"latitude\", None), 7)\n",
    "                longitude = round(step[\"location\"].get(\"longitude\", None),7)\n",
    "                if latitude is not None and longitude is not None:\n",
    "                    stops.append((placename, latitude, longitude))\n",
    "        \n",
    "        if not stops:\n",
    "            stops = [\"MANUAL\"]\n",
    "        \n",
    "        return city1, city2, stops\n",
    "    except (IndexError, AttributeError, ValueError) as e:\n",
    "        print(f\"Error extracting stop data from text: {e}\")\n",
    "        return None, None, [\"MANUAL\"]\n",
    "\n",
    "# Dictionary to store stops data\n",
    "stops_data = {}\n",
    "\n",
    "# Process each log file in the logs directory\n",
    "for log_filename in os.listdir(logs_directory):\n",
    "    if log_filename.endswith('.log'):\n",
    "        log_file_path = os.path.join(logs_directory, log_filename)\n",
    "        \n",
    "        with open(log_file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            entries = content.split('}\\n{')\n",
    "            \n",
    "            # Adjust entries to ensure they are valid JSON strings\n",
    "            entries = [entry + '}' if not entry.endswith('}') else entry for entry in entries]\n",
    "            entries = ['{' + entry if not entry.startswith('{') else entry for entry in entries]\n",
    "\n",
    "            # Extract stop data from each JSON entry\n",
    "            for entry in entries:\n",
    "                if entry.strip():  # Ensure it's not an empty line\n",
    "                    city1, city2, stops = extract_stops_from_text(entry)\n",
    "                    if city1 and city2:\n",
    "                        stops_data[(city1, city2)] = stops\n",
    "\n",
    "# Function to merge stop data into the trips DataFrame\n",
    "def merge_stops_data(row):\n",
    "    key = (row['City_1'], row['City_2'])\n",
    "    stops = stops_data.get(key, [\"MANUAL\"])\n",
    "    \n",
    "    for i, stop in enumerate(stops):\n",
    "        stop_index = i + 1\n",
    "        if isinstance(stop, tuple):\n",
    "            row[f'{stop_index}_stop'] = stop[0]\n",
    "            row[f'{stop_index}_stop_lat'] = stop[1]\n",
    "            row[f'{stop_index}_stop_lon'] = stop[2]\n",
    "        else:\n",
    "            row['1_stop'] = stop  # \"MANUAL\"\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply the merge function to the DataFrame\n",
    "trips_df = trips_df.apply(merge_stops_data, axis=1)\n",
    "\n",
    "# Reorder the columns so that stops are in the correct order\n",
    "stop_columns = sorted([col for col in trips_df.columns if re.match(r'^\\d+_stop$', col)], key=lambda x: int(x.split('_')[0]))\n",
    "stop_lat_columns = sorted([col for col in trips_df.columns if re.match(r'^\\d+_stop_lat$', col)], key=lambda x: int(x.split('_')[0]))\n",
    "stop_lon_columns = sorted([col for col in trips_df.columns if re.match(r'^\\d+_stop_lon$', col)], key=lambda x: int(x.split('_')[0]))\n",
    "\n",
    "# Combine stop name, latitude, and longitude columns in the correct order\n",
    "stop_columns_ordered = sum([[stop_columns[i], stop_lat_columns[i], stop_lon_columns[i]] for i in range(len(stop_columns))], [])\n",
    "\n",
    "# Final column order\n",
    "ordered_columns = ['ID', 'City_1', 'City_2', 'Duration_train', 'Train_CO2_kg', 'Plane_CO2_kg'] + stop_columns_ordered\n",
    "\n",
    "# Reorder DataFrame columns\n",
    "trips_df = trips_df[ordered_columns]\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "trips_df.to_csv(output_csv_file, index=False)\n",
    "\n",
    "print(f\"Data has been merged and saved to {output_csv_file}.\")"
   ],
   "id": "1c6f282bf0793f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#coordinates to lines in GeoJSON\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "csv_file = 'data/intermediate_stops_data.csv'\n",
    "output_dir = 'geojson_files'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    city1 = row[\"City_1\"]\n",
    "    city2 = row[\"City_2\"]\n",
    "    \n",
    "    # Prepare coordinates list\n",
    "    coordinates = []\n",
    "    \n",
    "    # Iterate through possible stops (up to 13 stops based on your CSV)\n",
    "    for i in range(1, 14):  # Stops from 1 to 13\n",
    "        lat_key = f\"Stop_{i}_lat\"\n",
    "        lon_key = f\"Stop_{i}_lon\"\n",
    "        \n",
    "        if pd.notna(row[lat_key]) and pd.notna(row[lon_key]):\n",
    "            coordinates.append([row[lon_key], row[lat_key]])\n",
    "    \n",
    "    if len(coordinates) > 1:\n",
    "        # Create GeoJSON structure\n",
    "        geojson_data = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"LineString\",\n",
    "                        \"coordinates\": coordinates\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"City_1\": city1,\n",
    "                        \"City_2\": city2\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Generate filename\n",
    "        filename = f\"{city1.replace(' ', '_')}_{city2.replace(' ', '_')}.geojson\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to GeoJSON file\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(geojson_data, f, indent=4)\n",
    "        \n",
    "        print(f\"GeoJSON file saved: {filepath}\")\n",
    "    else:\n",
    "        print(f\"Skipping {city1} to {city2}: Not enough valid coordinates\")\n",
    "\n",
    "print(\"GeoJSON files generated successfully.\")"
   ],
   "id": "8970340cfdb60467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#lines+points in GeoJSON\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "csv_file = 'data/trips_data_with_stops_final.csv'\n",
    "output_dir = 'geojson_files'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file, sep=';')\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    city1 = row[\"City_1\"]\n",
    "    city2 = row[\"City_2\"]\n",
    "    \n",
    "    # Prepare coordinates list for LineString\n",
    "    line_coordinates = []\n",
    "    \n",
    "    # Prepare list of Point features\n",
    "    point_features = []\n",
    "    \n",
    "    # Iterate through possible stops (up to 25 stops)\n",
    "    for i in range(1, 26):  # Stops from 1 to 25\n",
    "        stop_key = f\"{i}_stop\"\n",
    "        lat_key = f\"{i}_stop_lat\"\n",
    "        lon_key = f\"{i}_stop_lon\"\n",
    "        \n",
    "        if pd.notna(row[lat_key]) and pd.notna(row[lon_key]):\n",
    "            # Add to LineString coordinates\n",
    "            line_coordinates.append([row[lon_key], row[lat_key]])\n",
    "            \n",
    "            # Create a point feature\n",
    "            point_feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [row[lon_key], row[lat_key]]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"Start\": city1,\n",
    "                    \"End\": city2,\n",
    "                    \"stop_name\": row[stop_key],\n",
    "                    \"latitude\": row[lat_key],\n",
    "                    \"longitude\": row[lon_key]\n",
    "                }\n",
    "            }\n",
    "            point_features.append(point_feature)\n",
    "    \n",
    "    if len(line_coordinates) > 1:\n",
    "        # Create GeoJSON structure\n",
    "        geojson_data = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"LineString\",\n",
    "                        \"coordinates\": line_coordinates\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"Start\": city1,\n",
    "                        \"End\": city2\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add point features to GeoJSON\n",
    "        geojson_data[\"features\"].extend(point_features)\n",
    "        \n",
    "        # Generate filename\n",
    "        filename = f\"{city1.replace(' ', '_')}_{city2.replace(' ', '_')}.geojson\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to GeoJSON file\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(geojson_data, f, indent=4)\n",
    "        \n",
    "        print(f\"GeoJSON file saved: {filepath}\")\n",
    "    else:\n",
    "        print(f\"Skipping {city1} to {city2}: Not enough valid coordinates\")\n",
    "\n",
    "print(\"GeoJSON files generated successfully.\")"
   ],
   "id": "2684dde13a274638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#lines and points separately\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# File paths\n",
    "csv_file = 'data/trips_data_with_stops_final.csv'\n",
    "lines_dir = 'geojson_files/lines'\n",
    "points_dir = 'geojson_files/points'\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(lines_dir, exist_ok=True)\n",
    "os.makedirs(points_dir, exist_ok=True)\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_file, sep=';')\n",
    "\n",
    "# Iterate through each row in the DataFrame\n",
    "for _, row in df.iterrows():\n",
    "    city1 = row[\"City_1\"]\n",
    "    city2 = row[\"City_2\"]\n",
    "    \n",
    "    # Prepare coordinates list for LineString\n",
    "    line_coordinates = []\n",
    "    \n",
    "    # Prepare list of Point features\n",
    "    point_features = []\n",
    "    \n",
    "    # Iterate through possible stops (up to 25 stops)\n",
    "    for i in range(1, 26):\n",
    "        stop_key = f\"{i}_stop\"\n",
    "        lat_key = f\"{i}_stop_lat\"\n",
    "        lon_key = f\"{i}_stop_lon\"\n",
    "        \n",
    "        if pd.notna(row[lat_key]) and pd.notna(row[lon_key]):\n",
    "            # Add to LineString coordinates\n",
    "            line_coordinates.append([row[lon_key], row[lat_key]])\n",
    "            \n",
    "            # Create a point feature\n",
    "            point_feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [row[lon_key], row[lat_key]]\n",
    "                },\n",
    "                \"properties\": {\n",
    "                    \"Start\": city1,\n",
    "                    \"End\": city2,\n",
    "                    \"stop_name\": row[stop_key],\n",
    "                    \"latitude\": row[lat_key],\n",
    "                    \"longitude\": row[lon_key]\n",
    "                }\n",
    "            }\n",
    "            point_features.append(point_feature)\n",
    "    \n",
    "    # Generate filenames\n",
    "    base_filename = f\"{city1.replace(' ', '_')}_{city2.replace(' ', '_')}.geojson\"\n",
    "    \n",
    "    # Create LineString GeoJSON if there are enough coordinates\n",
    "    if len(line_coordinates) > 1:\n",
    "        line_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": [\n",
    "                {\n",
    "                    \"type\": \"Feature\",\n",
    "                    \"geometry\": {\n",
    "                        \"type\": \"LineString\",\n",
    "                        \"coordinates\": line_coordinates\n",
    "                    },\n",
    "                    \"properties\": {\n",
    "                        \"Start\": city1,\n",
    "                        \"End\": city2\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        line_filepath = os.path.join(lines_dir, base_filename)\n",
    "        with open(line_filepath, 'w') as f:\n",
    "            json.dump(line_geojson, f, indent=4)\n",
    "        print(f\"LineString GeoJSON saved: {line_filepath}\")\n",
    "    \n",
    "    # Create Points GeoJSON if there are valid points\n",
    "    if point_features:\n",
    "        points_geojson = {\n",
    "            \"type\": \"FeatureCollection\",\n",
    "            \"features\": point_features\n",
    "        }\n",
    "        points_filepath = os.path.join(points_dir, base_filename)\n",
    "        with open(points_filepath, 'w') as f:\n",
    "            json.dump(points_geojson, f, indent=4)\n",
    "        print(f\"Points GeoJSON saved: {points_filepath}\")\n",
    "\n",
    "print(\"GeoJSON files generated successfully.\")"
   ],
   "id": "ab3d6354098dd9aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#filter transfer points that are too close to each other (<5km)\n",
    "import os\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# File paths\n",
    "input_dir = 'geojson_files/points'\n",
    "output_dir = 'geojson_files/points_filtered'\n",
    "\n",
    "# Ensure output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Function to calculate distance between two coordinates\n",
    "def calculate_distance(coord1, coord2):\n",
    "    return geodesic(coord1, coord2).kilometers\n",
    "\n",
    "# Function to filter points within 5km of start and end\n",
    "def filter_points(geojson_data):\n",
    "    features = geojson_data[\"features\"]\n",
    "    \n",
    "    if not features:\n",
    "        return geojson_data\n",
    "\n",
    "    # Identify start and end points\n",
    "    start_point = features[0][\"geometry\"][\"coordinates\"]\n",
    "    end_point = features[-1][\"geometry\"][\"coordinates\"]\n",
    "\n",
    "    filtered_features = []\n",
    "    start_added = False\n",
    "    end_added = False\n",
    "\n",
    "    for feature in features:\n",
    "        point = feature[\"geometry\"][\"coordinates\"]\n",
    "        \n",
    "        # Keep the first point near the start\n",
    "        if not start_added and calculate_distance(point, start_point) <= 5:\n",
    "            filtered_features.append(feature)\n",
    "            start_added = True\n",
    "        \n",
    "        # Keep points that are not within 5km of the start or end\n",
    "        elif calculate_distance(point, start_point) > 5 and calculate_distance(point, end_point) > 5:\n",
    "            filtered_features.append(feature)\n",
    "        \n",
    "        # Keep the last point near the end\n",
    "        if not end_added and calculate_distance(point, end_point) <= 5:\n",
    "            end_added = True\n",
    "\n",
    "    # Ensure the final end point is added\n",
    "    if not end_added:\n",
    "        filtered_features.append(features[-1])\n",
    "    \n",
    "    return {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": filtered_features\n",
    "    }\n",
    "\n",
    "# Process each GeoJSON file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.geojson'):\n",
    "        input_filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Load the GeoJSON data\n",
    "        with open(input_filepath, 'r') as f:\n",
    "            geojson_data = json.load(f)\n",
    "        \n",
    "        # Filter points and generate new GeoJSON\n",
    "        filtered_geojson = filter_points(geojson_data)\n",
    "        \n",
    "        # Save the filtered GeoJSON to the output directory\n",
    "        output_filepath = os.path.join(output_dir, filename)\n",
    "        with open(output_filepath, 'w') as f:\n",
    "            json.dump(filtered_geojson, f, indent=4)\n",
    "        \n",
    "        print(f\"Filtered GeoJSON file saved: {output_filepath}\")\n",
    "\n",
    "print(\"Filtered GeoJSON files generated successfully.\")"
   ],
   "id": "251680994e1f5d29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def filter_points(input_dir='geojson_files/points', output_dir='geojson_files/points_filtered', buffer_km=5):\n",
    "    # Ensure output directory exists\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Iterate through each GeoJSON file in the input directory\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.geojson'):\n",
    "            input_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Load the GeoJSON file\n",
    "            with open(input_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            if data['features']:\n",
    "                # Get the first point (start) and the last point (end)\n",
    "                start_point = data['features'][0]\n",
    "                end_point = data['features'][-1]\n",
    "\n",
    "                start_coords = start_point['geometry']['coordinates']\n",
    "                end_coords = end_point['geometry']['coordinates']\n",
    "\n",
    "                filtered_features = []\n",
    "                added_start = False\n",
    "                added_end = False\n",
    "\n",
    "                # Iterate through the features and filter based on distance from start and end points\n",
    "                for feature in data['features']:\n",
    "                    coordinates = feature['geometry']['coordinates']\n",
    "                    distance_from_start = geodesic(start_coords[::-1], coordinates[::-1]).km\n",
    "                    distance_from_end = geodesic(end_coords[::-1], coordinates[::-1]).km\n",
    "\n",
    "                    if distance_from_start > buffer_km and distance_from_end > buffer_km:\n",
    "                        filtered_features.append(feature)\n",
    "                    elif distance_from_start <= buffer_km and not added_start:\n",
    "                        filtered_features.append(start_point)  # Ensure the first point (start) is kept\n",
    "                        added_start = True\n",
    "                    elif distance_from_end <= buffer_km and not added_end:\n",
    "                        filtered_features.append(end_point)  # Ensure the last point (end) is kept\n",
    "                        added_end = True\n",
    "\n",
    "                # Update the filtered GeoJSON data\n",
    "                data['features'] = filtered_features\n",
    "\n",
    "                # Save the filtered GeoJSON\n",
    "                output_path = os.path.join(output_dir, filename)\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(data, f, indent=4)\n",
    "\n",
    "                print(f\"Filtered GeoJSON file saved: {output_path}\")\n",
    "\n",
    "filter_points()"
   ],
   "id": "a4a273728b52090a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def filter_points(input_dir='geojson_files/points', output_dir='geojson_files/points_filtered_stopnum', buffer_km=5):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".geojson\"):\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            \n",
    "            with open(filepath, 'r') as f:\n",
    "                geojson_data = json.load(f)\n",
    "            \n",
    "            # Separate start and end city\n",
    "            start_city = geojson_data[\"features\"][0][\"properties\"][\"Start\"]\n",
    "            end_city = geojson_data[\"features\"][0][\"properties\"][\"End\"]\n",
    "            \n",
    "            # Get the coordinates for start and end points\n",
    "            start_point = geojson_data[\"features\"][0][\"geometry\"][\"coordinates\"]\n",
    "            end_point = geojson_data[\"features\"][-1][\"geometry\"][\"coordinates\"]\n",
    "            \n",
    "            filtered_features = []\n",
    "            stop_number = 1  # Initialize stop number\n",
    "            \n",
    "            # Loop through each point feature\n",
    "            for i, feature in enumerate(geojson_data[\"features\"]):\n",
    "                coordinates = feature[\"geometry\"][\"coordinates\"]\n",
    "                stop_name = feature[\"properties\"][\"stop_name\"]\n",
    "                \n",
    "                # Calculate distances to start and end points\n",
    "                dist_to_start = geodesic(start_point[::-1], coordinates[::-1]).km\n",
    "                dist_to_end = geodesic(end_point[::-1], coordinates[::-1]).km\n",
    "                \n",
    "                # Keep the first point in the start city, all non-start/end city points, and the last point\n",
    "                if (dist_to_start > buffer_km or i == 0) and (dist_to_end > buffer_km or i == len(geojson_data[\"features\"]) - 1):\n",
    "                    # Add stop_number to the properties\n",
    "                    feature[\"properties\"][\"stop_number\"] = stop_number\n",
    "                    stop_number += 1\n",
    "                    filtered_features.append(feature)\n",
    "            \n",
    "            # Update the GeoJSON structure\n",
    "            geojson_data[\"features\"] = filtered_features\n",
    "            \n",
    "            # Save the filtered GeoJSON file\n",
    "            output_filepath = os.path.join(output_dir, filename)\n",
    "            with open(output_filepath, 'w') as f:\n",
    "                json.dump(geojson_data, f, indent=4)\n",
    "            \n",
    "            print(f\"Filtered GeoJSON saved: {output_filepath}\")\n",
    "\n",
    "filter_points()"
   ],
   "id": "78585f90478b2512",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Function to remove redundant points within 5 km for all stops along the route\n",
    "def filter_redundant_points_full_route(input_filepath, output_filepath):\n",
    "    with open(input_filepath, 'r') as f:\n",
    "        geojson_data = json.load(f)\n",
    "    \n",
    "    features = geojson_data[\"features\"]\n",
    "    filtered_features = []\n",
    "    \n",
    "    # Function to check if two points are within the 5 km radius\n",
    "    def is_within_5km(coord1, coord2):\n",
    "        return geodesic(coord1, coord2).km <= 5\n",
    "    \n",
    "    # Keep track of the last added point to filter redundant points\n",
    "    last_point = None\n",
    "    \n",
    "    # Iterate through all points\n",
    "    for feature in features:\n",
    "        if feature[\"geometry\"][\"type\"] == \"Point\":\n",
    "            current_point = feature[\"geometry\"][\"coordinates\"]\n",
    "            \n",
    "            if last_point is None or not is_within_5km(current_point, last_point):\n",
    "                \n",
    "                # Add the point to the filtered list and update the last_point\n",
    "                filtered_features.append(feature)\n",
    "                last_point = current_point\n",
    "    \n",
    "    # Update the GeoJSON data with filtered features\n",
    "    geojson_data[\"features\"] = filtered_features\n",
    "    \n",
    "    # Save the filtered points to a new GeoJSON file\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(geojson_data, f, indent=4)\n",
    "\n",
    "# Directories\n",
    "input_dir = 'geojson_files/points/'\n",
    "output_dir = 'geojson_files/points_new/'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process all GeoJSON files in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".geojson\"):  # Only process GeoJSON files\n",
    "        input_filepath = os.path.join(input_dir, filename)\n",
    "        output_filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Filter redundant points for each route\n",
    "        filter_redundant_points_full_route(input_filepath, output_filepath)\n",
    "        print(f\"Processed: {filename}\")\n",
    "\n",
    "print(\"All files processed and saved successfully.\")"
   ],
   "id": "fc141b5211a36142",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c73b4b4b1898545",
   "metadata": {},
   "source": "## Alternative data source - [EcoPassenger](https://ecopassenger.hafas.de/bin/query.exe/en?L=vs_uic) parser (not used in the end)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://ecopassenger.hafas.de/bin/query.exe/en?ld=uic-eco&L=vs_uic&protocol=https:&seqnr=1&ident=nt.0241101.1719352628&REQ0HafasScrollDir=1&ecocon=C1-0\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "#Parse HTML using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#Find table row with CO2 data\n",
    "rows = soup.find_all('tr')\n",
    "for row in rows:\n",
    "    if row.find('td', class_='sepline nowrap') and 'Carbon dioxide' in row.get_text():\n",
    "        co2_row = row\n",
    "        break\n",
    "\n",
    "#Extract CO2 values for train and plane\n",
    "train_co2 = co2_row.find_all('td')[1].text.strip()\n",
    "plane_co2 = co2_row.find_all('td')[5].text.strip()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Train CO2: {train_co2}\")\n",
    "print(f\"Plane CO2: {plane_co2}\")\n"
   ],
   "id": "b2e1318764e5446f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_first_file(file_path):\n",
    "    city_pairs = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if \"completed:\" in line:\n",
    "                # Extract the part after the \"completed:\" keyword\n",
    "                part = line.split(\"completed:\")[1].strip()\n",
    "                # Extract the city pair\n",
    "                cities = part.split(\" to \")\n",
    "                if len(cities) == 2:\n",
    "                    city_pairs.add((cities[0].strip(), cities[1].strip()))\n",
    "    return city_pairs\n",
    "\n",
    "def parse_second_file(file_path):\n",
    "    city_pairs = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Each line is in the format \"City1,City2,...\"\n",
    "            parts = line.strip().split(\",\")\n",
    "            if len(parts) >= 2:\n",
    "                city_pairs.add((parts[0].strip(), parts[1].strip()))\n",
    "    return city_pairs\n",
    "\n",
    "def find_missing_pairs(first_file_path, second_file_path):\n",
    "    first_file_pairs = parse_first_file(first_file_path)\n",
    "    second_file_pairs = parse_second_file(second_file_path)\n",
    "    \n",
    "    # Find the difference\n",
    "    missing_pairs = first_file_pairs - second_file_pairs\n",
    "    return missing_pairs\n",
    "\n",
    "# Replace these with your actual file paths\n",
    "first_file_path = 'api_logs/misc/list_api_req.txt'\n",
    "second_file_path = 'data/trips_data.csv'\n",
    "\n",
    "missing_pairs = find_missing_pairs(first_file_path, second_file_path)\n",
    "\n",
    "# Print missing pairs\n",
    "for pair in missing_pairs:\n",
    "    print(f\"Missing pair: {pair[0]} to {pair[1]}\")"
   ],
   "id": "ab3012c96bc540a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pip install selenium",
   "id": "44b0ee49a1e11376"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Set up the WebDriver using ChromeDriverManager\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--lang=en-UK\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "#Input values\n",
    "from_location = 'PARIS NORD (France)'\n",
    "to_location = 'BERLIN (Germany)'\n",
    "travel_date = '01.07.24'\n",
    "travel_time = '06:00'\n",
    "\n",
    "try:\n",
    "    #Navigate to the EcoPassenger search page\n",
    "    driver.get('https://ecopassenger.hafas.de/bin/query.exe/en?L=vs_uic&')\n",
    "\n",
    "    from_input = driver.find_element(By.ID, 'from')\n",
    "    from_input.clear()\n",
    "    from_input.send_keys(from_location)\n",
    "\n",
    "    to_input = driver.find_element(By.ID, 'to')\n",
    "    to_input.clear()\n",
    "    to_input.send_keys(to_location)\n",
    "\n",
    "    date_input = driver.find_element(By.ID, 'date')\n",
    "    date_input.clear()\n",
    "    date_input.send_keys(travel_date)\n",
    "\n",
    "    time_input = driver.find_element(By.ID, 'time')\n",
    "    time_input.clear()\n",
    "    time_input.send_keys(travel_time)\n",
    "    \n",
    "    #Click the 'Start request' button\n",
    "    start_button = driver.find_element(By.CSS_SELECTOR, \"button[name='application=ECOLOGYINFO&start']\")\n",
    "    start_button.click()\n",
    "\n",
    "    #Wait for the results page to load and print the results\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, \".hafasEcology\"))\n",
    "    )\n",
    "    \n",
    "    #Pause for manual selection of the fastest train option\n",
    "    input(\"Press Enter to continue after you do your thing...\")\n",
    "    \n",
    "    #Get the page source and parse travel time and emissons for train and plane\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    durations = []\n",
    "    summary_table = soup.find('table', class_='result')\n",
    "    if summary_table:\n",
    "        duration_rows = summary_table.find_all('td', class_='sepline borderright top')\n",
    "        for duration_row in duration_rows:\n",
    "            if duration_row.find('div', class_='lc_th') and 'Duration' in duration_row.find('div', class_='lc_th').text:\n",
    "                duration = duration_row.text.split()[-1].strip()\n",
    "                durations.append(duration)\n",
    "    \n",
    "    duration_train, _, duration_plane = durations\n",
    "    \n",
    "    rows = soup.find_all('tr')\n",
    "    co2_row = None\n",
    "    for row in rows:\n",
    "        if row.find('td', class_='sepline nowrap') and 'Carbon dioxide' in row.get_text():\n",
    "            co2_row = row\n",
    "            break\n",
    "\n",
    "    if co2_row:\n",
    "        train_co2 = co2_row.find_all('td')[1].text.strip()\n",
    "        plane_co2 = co2_row.find_all('td')[5].text.strip()\n",
    "    else:\n",
    "        print(\"CO2 data row not found\")\n",
    "        \n",
    "    #Create a DataFrame for the results\n",
    "    data = {\n",
    "        \"Start\": [from_location],\n",
    "        \"End\": [to_location],\n",
    "        \"Duration_train\": [duration_train],\n",
    "        \"Train_CO2_kg\": [train_co2],\n",
    "        \"Duration_plane\": [duration_plane],\n",
    "        \"Plane_CO2_kg\": [plane_co2],\n",
    "        \"Date\": [travel_date]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n",
    "    #df.to_csv('train_plane_emissions_durations.csv', index=False)\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ],
   "id": "98af95625978c5ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
